learning rate, data processing, overfitting

1
gradient descent할때
알파값 랜덤으로 정함

2
learning rate를 큰값으로 정하게 되면
overshooting 팅겨나가게 된다. (발산)
와리가리하는 모양

3
learning rate가 너무 적으면
최저점을 찾아가는데 시간이 너무 오래 걸린다.
이것을 확인해보는 방법은 cost function을 출력 해보는것이다.

4
0.01정도 높게 적게 변환

5
등고선을 그려봤을때 일정하게 줄어드는 것이
가장 이상적인 등고선이다.

6
값이 차이가 많이나는 데이터가 들어올경우
길쭉한 등고선모양의 gradient descent가 된다.
우리가 어떤 알파값을 잡았을 때
아무리 좋은 값이여 데이터가 차이 많이나면 팅겨나가게 된다.

7
그래서 normalize해줘야 한다.
0,0 으로 맞춰주고 범위안으로 줄이기
데이터 값들을 어떤범위안에 항상 들어가도록 하는 것

8
standardization
여러분이 계산한 평균과 분산을 가지고 나눔
preprocessing하는 것이다.

9
overfitting
학습모델에 너무 딱 맞는 모델만 만드는 것
실제데이터와 테스트데이터에 잘 맞지 않는 경우

10
오버피팅을 줄이는 법은
1. 트레이닝 데이터 셋을 많이 가지고 있는 것
2. feature의 개수 줄이기(중복된 것들)
3. regularization - 일반화

11
일반화
구분선을 구부리지말고 피자!
코스트 함수를 최소화 시키는것이 목표인데
코스트함수에 + 상수 * 시그마w2 = 각각의 엘리먼트를 제곱하는것
reqularization strength 라고 불림 0 ~ 1사이의 값
상수가 0이면 ㄴㄴ
1이면 중요
0.001 * tf.reduce_sum(tf.square(W))

12
요약
러닝레잇 중요이유 - 세팅방법
데이터 전처링
오버피팅

러닝레이트 같은 것을 hyper parameter라고 부른다.
grid search는 모든 러닝레이트의 가능성을 계산
mini-batch gradient descent를 자주 쓴다. 모든 데이터르 전부 계산하기 힘들기 때문에

