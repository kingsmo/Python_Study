multivariable linear regression

1
복습
hypothesis h = w(x) + y
w와 b를 학습함
cost function cost(w, b) = 1/m
gradient descent algorithm - cost 최적화

2
하나의 x와 y면 여태까지 배운것을 하면 된다.

3
but 여러가지 x가 생긴다
퀴즈 1, 퀴즈 2, 중간 -> 기말

4
hypothesis는 넓게만 해주면 됨
h(x1, x2, x3) = w1x1 + w2x2 + w3x3 + b

5
cost는 똑같다

6
n개여도 똑같다!

7
n개면 너무 넓어진다
matrix를 사용하자

8
matrix곱셈!을 이용

9
w1x1 + w2x2 + w3x3 + ... wnxn
(x1, x2 x3) (w1
            w2  =  x1w1 + x2w2 + x3w3
            w3)
요런식으로 사용
매트릭스를 쓸때는 보통 x를 앞에둠
그리고 대문자를 씀
H(X) = XW

10
many x instances
매트릭스 곱 일일이 해주기 귀찮음

11
인스턴스의 수 만큼 row개수를 주면 된다.
매트릭스의 장점이다!

12
매트릭스 곱
5x3 * 3x1 = 5x1이 된다.

13
5x3에서 5는 instance개수 3은 variable 개수
결과 값은 instance개수 x 1이다.
X * W = H(X)
W의 크기를 쉽게 알 수 있음!

14
none은 여러개 즉, n개 들어올 수 있다는 뜻

15
매트릭스로 multivariable, n개의 instances 쉽게 처리 가능

16
lecture와 implementation의 차이인데
수학적으론 결국 같음
