Bias ans Variance

에러에 대해 이해를 먼저하자
두가지 approximation and generalization
approximation + 오메가
오메가는 앞으로 올 제너럴한 부분에 대한 에러

f : target function
g : the learning function of ML
g(d) : 데이터셋을 적용시킨 ml model
D : 데이터 셋
g햇 : averave hypothesis
    매번 데이터가 샘플링 할 때마다 평균으로 만든 것이다,

들어올 수 있는 expectation
expected error
식풀기...

variance(x) = g(d)(x) - g(x)의 제곱을 variance term
bias(x) = g햇(x) - f(x)의 제곱
두가지는 트레이드 오프 관계이다.

variance는 average hypothesis로 훈련할 수 없다. 데이터셋때문에
bias는 average hypothesis를 현실세계에 매치할 수 없음
varaince reducing은 더 많은 데이터를 수집
bias reducing은 더 복잡한 모델
